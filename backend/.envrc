#!/usr/bin/env bash
# Default environment variables for local development
# For production credentials or environment-specific overrides, create .envrc.override
# .envrc.override is gitignored and will not be committed to version control

set -euo pipefail
__prevEnv__="$(env)"

DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" >/dev/null 2>&1 && pwd)"
export NX_WORKSPACE_ROOT=${DIR}

PATH_add node_modules/.bin
PATH_add tools/bin
PATH_add packages/dev/database/bin

export HASURA_VERSION="2.35.1"
export NATS_VERSION="0.1.1"

export FIXTURE_DIR="${NX_WORKSPACE_ROOT}/tests/fixtures"

export WORKSPACE_NAME="kedge"
export DEV_NETWORK_NAME="kedge_network"
export HASURA_GRAPHQL_SERVER_PORT="28717"
export HASURA_GRAPHQL_ADMIN_SECRET="9AgJckEMHPRgrasj7Ey8jR"
export HASURA_ENDPOINT="http://localhost:28717"
export HASURA_SECRET="9AgJckEMHPRgrasj7Ey8jR"

# Default to local database - override in .envrc.override for other environments
export NODE_DATABASE_URL="postgres://postgres:postgres@127.0.0.1:7543/kedge_db"
export HASURA_GRAPHQL_DATABASE_URL="postgres://postgres:postgres@127.0.0.1:7543/kedge_db"

export JWT_SECRET="111"
export REDIS_DB=0
export REDIS_HOST=localhost
export REDIS_PORT=6379

export API_PORT=8718
export ENABLE_DEBUG_INFO=true
export LOG_LEVEL=debug
export NODE_ENV=development
export QUIZ_STORAGE_PATH="./quiz-storage"

# LLM API Configuration
# The system automatically detects the provider based on model name:
# - Models starting with "gpt-", "o1-", "chatgpt-" → OpenAI
# - Models starting with "deepseek-" → DeepSeek
export LLM_API_KEY="your-llm-api-key-here"

# Optional: Override the base URL (auto-detected based on model if not set)
# export LLM_BASE_URL="https://api.openai.com/v1"  # OpenAI
# export LLM_BASE_URL="https://api.deepseek.com"   # DeepSeek

# Optional: Organization ID for OpenAI
# export LLM_ORGANIZATION="your-org-id"

# Model Configuration for Different Use Cases
# Quiz Parser - Complex task of extracting quiz items from documents
export LLM_MODEL_QUIZ_PARSER="gpt-4o"          # Use "deepseek-chat" for DeepSeek
export LLM_TEMP_QUIZ_PARSER="0.1"              # Temperature: 0.7 (some creativity)
export LLM_MAX_TOKENS_QUIZ_PARSER="4000"       # Max output tokens: 4000

# Quiz Renderer - Simpler task of formatting/polishing quiz questions
export LLM_MODEL_QUIZ_RENDERER="gpt-4o-mini"   # Cost-effective model
export LLM_TEMP_QUIZ_RENDERER="0.3"            # Temperature: 0.3 (consistent)
export LLM_MAX_TOKENS_QUIZ_RENDERER="1000"     # Max output tokens: 1000

# Answer Validator - Semantic validation of fill-in-the-blank answers
export LLM_MODEL_ANSWER_VALIDATOR="gpt-4o-mini" # Fast validation model
export LLM_TEMP_ANSWER_VALIDATOR="0.1"         # Temperature: 0.3 (consistent)
export LLM_MAX_TOKENS_ANSWER_VALIDATOR="500"   # Max output tokens: 500

# Knowledge Point Extractor - Extract keywords and match knowledge points
export LLM_MODEL_KNOWLEDGE_EXTRACTOR="gpt-4o"  # Accurate extraction model
export LLM_TEMP_KNOWLEDGE_EXTRACTOR="0.1"      # Temperature: 0.3 (consistent)
export LLM_MAX_TOKENS_KNOWLEDGE_EXTRACTOR="1000" # Max output tokens: 1000


if [[ -f .envrc.override ]]; then
  source_env .envrc.override
fi

# ========================================
# Aliyun OSS Configuration (Optional)
# ========================================
# If configured, attachments will be stored in OSS instead of local filesystem
# Leave empty to use legacy file system storage

# Aliyun OSS Access Credentials
export ALIYUN_OSS_ACCESS_KEY_ID=""
export ALIYUN_OSS_ACCESS_KEY_SECRET=""

# OSS Bucket Configuration
export ALIYUN_OSS_BUCKET=""          # e.g., "kedge-quiz-attachments"
export ALIYUN_OSS_REGION=""          # e.g., "oss-cn-hangzhou"
export ALIYUN_OSS_ENDPOINT=""        # e.g., "oss-cn-hangzhou.aliyuncs.com"

# OSS Storage Options
export ALIYUN_OSS_INTERNAL_ENDPOINT="" # Optional: Internal endpoint for ECS (faster, no bandwidth charge)
export ALIYUN_OSS_CDN_DOMAIN=""        # Optional: CDN domain for faster access, e.g., "cdn.example.com"
export ALIYUN_OSS_PATH_PREFIX="quiz-attachments" # Prefix for all uploaded files
export ALIYUN_OSS_PUBLIC_READ="false"  # Whether uploaded files should be publicly readable

# export updated ENV of this file
node "${NX_WORKSPACE_ROOT}/tools/bin/get-env" "${__prevEnv__}" "$(env)" >"${NX_WORKSPACE_ROOT}/.env" &
